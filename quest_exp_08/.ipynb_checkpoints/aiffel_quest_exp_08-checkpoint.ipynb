{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0546737",
   "metadata": {},
   "source": [
    "# 프로젝트: 한국어 데이터로 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d165f99",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e612723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d164d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ChatbotData.csv')\n",
    "print(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f907b03",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기 & Step 3. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa689fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    # 단어와 구두점(punctuation) 사이의 거리 만들기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # (숫자, 한글, 한글 자음, 알파벳 대소문자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체\n",
    "    sentence = re.sub(r'[^0-9A-Za-z가-힣ㄱ-ㅎ.,?! ]', ' ', sentence)\n",
    "    # 숫자 앞에 숫자 아닌 문자가 있으면 공백을 추가\n",
    "    sentence = re.sub(r'(?<=\\D)(?=\\d)(?<! )', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9b2392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12월 19일부터 3박 4일 놀러가고 싶다 . ㅋㅋㅋㅋ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence('12월 19일부터 3박4일 놀러가고 싶다. ㅋㅋㅋㅋ~ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1afa077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박 4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박 4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임 .</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠 !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임 .</td>\n",
       "      <td>훔쳐보는 거 티나나봐요 .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남 .</td>\n",
       "      <td>설렜겠어요 .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까 ?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요 .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요 .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Q                          A  label\n",
       "0                       12시 땡 !                하루가 또 가네요 .      0\n",
       "1                   1지망 학교 떨어졌어                 위로해 드립니다 .      0\n",
       "2                 3박 4일 놀러가고 싶다               여행은 언제나 좋죠 .      0\n",
       "3              3박 4일 정도 놀러가고 싶다               여행은 언제나 좋죠 .      0\n",
       "4                       PPL 심하네                눈살이 찌푸려지죠 .      0\n",
       "...                         ...                        ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임 .        티가 나니까 눈치가 보이는 거죠 !      2\n",
       "11819           훔쳐보는 것도 눈치 보임 .             훔쳐보는 거 티나나봐요 .      2\n",
       "11820              흑기사 해주는 짝남 .                    설렜겠어요 .      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까 ?  잘 헤어질 수 있는 사이 여부인 거 같아요 .      2\n",
       "11822                힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요 .      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q'] = data['Q'].apply(preprocess_sentence)\n",
    "data['A'] = data['A'].apply(preprocess_sentence)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35208a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = data['Q'].values, data['A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48531ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 샘플 수: 11823\n",
      "답변 샘플 수: 11823\n"
     ]
    }
   ],
   "source": [
    "print(f'질문 샘플 수: {len(inputs)}')\n",
    "print(f'답변 샘플 수: {len(outputs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1792aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(inputs + outputs, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad32e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8360]\n",
      "END_TOKEN의 번호 : [8361]\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6edc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc3694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 전의 21번째 질문 샘플: 가스비 장난 아님\n",
      "정수 인코딩 전의 21번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n",
      "정수 인코딩 후의 21번째 질문 샘플: [5824, 606, 2497, 4170]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2681, 7665, 8, 6371, 95, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "\n",
    "print('정수 인코딩 전의 21번째 질문 샘플: {}'.format(inputs[21]))\n",
    "print('정수 인코딩 전의 21번째 답변 샘플: {}'.format(outputs[21]))\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(inputs[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(outputs[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99828e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가스 => 5824\n",
      "비  => 606\n",
      "장난  => 2497\n",
      "아님 => 4170\n",
      "다음  => 2681\n",
      "달에는  => 7665\n",
      "더  => 8\n",
      "절약해 => 6371\n",
      "봐요 => 95\n",
      " . => 1\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 관계 확인\n",
    "for token in tokenizer.encode(inputs[21]) + tokenizer.encode(outputs[21]):\n",
    "    print(f'{tokenizer.decode([token])} => {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1aa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b402c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_count_sentence_length(sentences, MAX_LENGTH=MAX_LENGTH):\n",
    "    sentences_len = np.array([len(sen) for sen in sentences])\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(sentences_len)\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(sentences_len)\n",
    "    count = 0\n",
    "    for length in sentences_len:\n",
    "        if length <= MAX_LENGTH:\n",
    "            count += 1\n",
    "    print(f'전체 문장 중 길이가 MAX_LEN 이하인 문장의 비율: {100*count/len(sentences)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b99204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장 중 길이가 MAX_LEN 이하인 문장의 비율: 87.1098705912205%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_count_sentence_length(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ccf08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장 중 길이가 MAX_LEN 이하인 문장의 비율: 80.30110800981139%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_count_sentence_length(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bab6698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac2dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs, MAX_LENGTH=MAX_LENGTH):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        len1 = len(sentence1)\n",
    "        len2 = len(sentence2)\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 인코딩 이전 문장의 길이가 MAX_LENGTH 이하인 경우에만 데이터셋으로 허용\n",
    "        if len1 <= MAX_LENGTH and len2 <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # 최대 길이 MAX_LENGTH로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cbb8dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8362\n",
      "필터링 후의 질문 샘플 개수: 8486\n",
      "필터링 후의 답변 샘플 개수: 8486\n"
     ]
    }
   ],
   "source": [
    "inputs_tkned, outputs_tkned = tokenize_and_filter(inputs, outputs)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(inputs_tkned)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(outputs_tkned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028b2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교사 강요를 위한 전처리\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': inputs_tkned,\n",
    "        'dec_inputs': outputs_tkned[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': outputs_tkned[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a26801",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2000fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab79288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "  \n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f372f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cab9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46f01ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14f792fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "      outputs = decoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name='decoder_layer_{}'.format(i),\n",
    "      )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d85af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67c49500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3194880     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3722240     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8362)   2149034     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,066,154\n",
      "Trainable params: 9,066,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70540408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4ff3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a272769",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53e13d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "      # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "      predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "      predictions = predictions[:, -1:, :]\n",
    "\n",
    "      # 현재 예측한 단어의 정수\n",
    "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "      # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "      if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "        break\n",
    "\n",
    "      # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "      # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "      output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eff559d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "    if predicted_sentence[-2:] == ' .':\n",
    "        predicted_sentence = predicted_sentence[:-2] + '.'\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157482d",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52ca83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 8s 38ms/step - loss: 2.6316 - accuracy: 0.0535\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 2.2432 - accuracy: 0.0996\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 37ms/step - loss: 1.8645 - accuracy: 0.1019\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 1.6471 - accuracy: 0.1033\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 1.5418 - accuracy: 0.1070\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 1.4585 - accuracy: 0.1123\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 1.3740 - accuracy: 0.1179\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 1.2824 - accuracy: 0.1268\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 1.1825 - accuracy: 0.1372\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 1.0692 - accuracy: 0.1512\n",
      "EPOCHS = 10\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 잘 지내고 있을 거예요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 저도 좋아해요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 저도 좋아해요.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 저도 좋아해요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 마음이 허전하겠어요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 제가 있잖아요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 제가 있잖아요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 10 에폭\n",
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 10')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9fbffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.9474 - accuracy: 0.1670\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.8197 - accuracy: 0.1832\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.6925 - accuracy: 0.2007\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.5673 - accuracy: 0.2183\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.4495 - accuracy: 0.2362\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.3438 - accuracy: 0.2537\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.2558 - accuracy: 0.2675\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.1833 - accuracy: 0.2804\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.1292 - accuracy: 0.2889\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0907 - accuracy: 0.2961\n",
      "EPOCHS = 20\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 안녕하세요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 저도 궁금해요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 지금 당장 거울 앞에 서 보세요.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 마음도 추운가요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 하늘 만큼 땅 만큼 사랑해요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 지금 그러고 있어요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 저랑 같이 놀아요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 20 에폭\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 20')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbbcff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0677 - accuracy: 0.2993\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0541 - accuracy: 0.3008\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.0451 - accuracy: 0.3017\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0423 - accuracy: 0.3019\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0378 - accuracy: 0.3028\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0383 - accuracy: 0.3024\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0373 - accuracy: 0.3023\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0342 - accuracy: 0.3031\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0350 - accuracy: 0.3025\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0348 - accuracy: 0.3025\n",
      "EPOCHS = 30\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 맘고생 많았어요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 좋았으면 좋겠네요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 정신 노동을 했나 봐요.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 마음도 추운가요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 하늘 만큼 땅 만큼 사랑해요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 또 다른 시작이기도 해요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 직접 물어보는 게 좋을 것 같아요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 30 에폭\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 30')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa52a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.0352 - accuracy: 0.3024\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0314 - accuracy: 0.3031\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0276 - accuracy: 0.3041\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0249 - accuracy: 0.3049\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0234 - accuracy: 0.3052\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0225 - accuracy: 0.3054\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0206 - accuracy: 0.3058\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0187 - accuracy: 0.3063\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0173 - accuracy: 0.3067\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0171 - accuracy: 0.3068\n",
      "EPOCHS = 40\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 안녕하세요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 좋았으면 좋겠네요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 좀 더 기다려주세요.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 마음도 추운가요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 하늘 만큼 땅 만큼 사랑해요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 이제 그만 놓아주세요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 같이 가요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 40 에폭\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 40')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74eaec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0156 - accuracy: 0.3073\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0152 - accuracy: 0.3071\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0140 - accuracy: 0.3077\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0134 - accuracy: 0.3077\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0126 - accuracy: 0.3079\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0121 - accuracy: 0.3080\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0104 - accuracy: 0.3084\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0114 - accuracy: 0.3082\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0107 - accuracy: 0.3083\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0111 - accuracy: 0.3082\n",
      "EPOCHS = 50\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 안녕하세요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 좋았으면 좋겠네요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 정신 노동을 했나 봐요.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 신경 쓸 일이 있나봐요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 상대방에게 전해보세요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 이제 그만 놓아주세요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 직접 물어보는 게 좋을 것 같아요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 50 에폭\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 50')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31d7c1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0100 - accuracy: 0.3084\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 5s 37ms/step - loss: 0.0101 - accuracy: 0.3084\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0088 - accuracy: 0.3086\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0091 - accuracy: 0.3087\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0081 - accuracy: 0.3088\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0081 - accuracy: 0.3088\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 5s 39ms/step - loss: 0.0082 - accuracy: 0.3088\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0087 - accuracy: 0.3087\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0074 - accuracy: 0.3090\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 5s 38ms/step - loss: 0.0074 - accuracy: 0.3092\n",
      "EPOCHS = 60\n",
      "=========\n",
      "입력 : 안녕?\n",
      "출력 : 안녕하세요.\n",
      "=========\n",
      "입력 : 오늘 기분은 어때?\n",
      "출력 : 저도 궁금해요.\n",
      "=========\n",
      "입력 : 나 지금 피곤해\n",
      "출력 : 좀 나아졌길 바랍니다.\n",
      "=========\n",
      "입력 : 오늘 날씨가 좋아\n",
      "출력 : 마음도 추운가요.\n",
      "=========\n",
      "입력 : 사랑해\n",
      "출력 : 상대방에게 전해보세요.\n",
      "=========\n",
      "입력 : 난 너가 싫어\n",
      "출력 : 이제 그만 놓아주세요.\n",
      "=========\n",
      "입력 : 심심할 때 뭘 하면 좋을까?\n",
      "출력 : 그럼요 . 좋을 거예요.\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "# 60 에폭\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "print('EPOCHS = 60')\n",
    "print('=========')\n",
    "for Q in ['안녕?', '오늘 기분은 어때?', '나 지금 피곤해', '오늘 날씨가 좋아', '사랑해', '난 너가 싫어', '심심할 때 뭘 하면 좋을까?']:\n",
    "    sentence_generation(Q)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262927f",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 텍스트 전처리는 한국어에 맞게 진행했다. 공백, 특수문자, 한글과 알파벳, 숫자에 대해 전처리를 하고 길이가 긴 샘플은 제거했다.\n",
    "- 트랜스포머 모델은 너무 복잡해서 학습 노드의 코드를 거의 그대로 가져왔다(ㅠㅠ). 추후에 더 공부를 해야한다.\n",
    "- 학습이 진행될수록 챗봇이 점점 더 그럴듯하게 반응하는 경향을 보였다. 어떤 질문에 대해서는 학습이 진행되면서 대답이 더 이상해졌다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
